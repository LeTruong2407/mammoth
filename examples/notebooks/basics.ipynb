{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ec72291",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script contains a simple example of how to use the Mammoth library.\n",
    "\n",
    "We will see:\n",
    "- How to load the necessary stuff to run a model on a particular dataset.\n",
    "- What arguments are available for the model and what are the required ones.\n",
    "- How to run a model.\n",
    "- How to save and load a model.\n",
    "\"\"\"\n",
    "\n",
    "from mammoth import train, load_runner, get_avail_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "706a8624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12-Jun-25 18:36:23 - Trying to load default configuration for model sgd but no configuration file found in None.\n",
      "Required arguments:\n",
      "  dataset: Which dataset to perform experiments on.\n",
      "  model: Model name.\n",
      "  lr: Learning rate. This should either be set as default by the model (with `set_defaults <https://docs.python.org/3/library/argparse.html#argparse.ArgumentParser.set_defaults>`_), by the dataset (with `set_default_from_args`, see :ref:`module-datasets.utils`), or with `--lr=<value>`.\n",
      "\n",
      "Optional arguments:\n",
      "  backbone: resnet18 - Backbone network name.\n",
      "  load_best_args: False - (deprecated) Loads the best arguments for each method, dataset and memory buffer. NOTE: This option is deprecated and not up to date.\n",
      "  dataset_config: None - The configuration used for this dataset (e.g., number of tasks, transforms, backbone architecture, etc.).The available configurations are defined in the `datasets/config/<dataset>` folder.\n",
      "  model_config: default - The configuration used for this model. The available configurations are defined in the `models/config/<model>.yaml` file and include a `default` (dataset-agostic) configuration and a `best` configuration (dataset-specific). If not provided, the `default` configuration is used.\n",
      "  transform_type: weak - \n",
      "  num_classes: None - \n",
      "  num_filters: 64 - \n",
      "  seed: None - The random seed. If not provided, a random seed will be used.\n",
      "  permute_classes: False - Permute classes before splitting into tasks? This applies the seed before permuting if the `seed` argument is present.\n",
      "  base_path: ./data/ - The base path where to save datasets, logs, results.\n",
      "  checkpoint_path: ./checkpoints/ - The path where to save the checkpoints.\n",
      "  results_path: results/ - The path where to save the results. NOTE: this path is relative to `base_path`.\n",
      "  device: None - The device (or devices) available to use for training. More than one device can be specified by separating them with a comma. If not provided, the code will use the least used GPU available (if there are any), otherwise the CPU. MPS is supported and is automatically used if no GPU is available and MPS is supported. If more than one GPU is available, Mammoth will use the least used one if `--distributed=no`.\n",
      "  notes: None - Helper argument to include notes for this run. Example: distinguish between different versions of a model and allow separation of results\n",
      "  eval_epochs: None - Perform inference on validation every `eval_epochs` epochs. If not provided, the model is evaluated ONLY at the end of each task.\n",
      "  non_verbose: False - Make progress bars non verbose\n",
      "  disable_log: False - Disable logging?\n",
      "  num_workers: None - Number of workers for the dataloaders (default=infer from number of cpus).\n",
      "  enable_other_metrics: False - Enable computing additional metrics: forward and backward transfer.\n",
      "  inference_only: False - Perform inference only for each task (no training).\n",
      "  code_optimization: 0 - Optimization level for the code.0: no optimization.1: Use TF32, if available.2: Use BF16, if available.3: Use BF16 and `torch.compile`. BEWARE: torch.compile may break your code if you change the model after the first run! Use with caution.\n",
      "  distributed: no - Enable distributed training?\n",
      "  savecheck: None - Save checkpoint every `task` or at the end of the training (`last`).\n",
      "  save_checkpoint_mode: safe - Save the model checkpoint with metadata in a single pickle file with the old structure (`old_pickle`) or with the new, `safe` structure (default)?. NOTE: the `old_pickle` structure requires `weights_only=False`, which will be deprecated by PyTorch.\n",
      "  loadcheck: None - Path of the checkpoint to load (.pt file for the specific task)\n",
      "  ckpt_name: None - (optional) checkpoint save name.\n",
      "  start_from: None - Task to start from\n",
      "  stop_after: None - Task limit\n",
      "  save_after_interrupt: True - Whether to save the model checkpoint after an interrupt - SigInt (default: True).\n",
      "  wandb_name: None - Wandb name for this run. Overrides the default name (`args.model`).\n",
      "  wandb_entity: None - Wandb entity\n",
      "  wandb_project: None - Wandb project name\n",
      "  batch_size: 32 - Batch size.\n",
      "  label_perc: 1.0 - Percentage in (0-1] of labeled examples per task.\n",
      "  label_perc_by_class: 1.0 - Percentage in (0-1] of labeled examples per task.\n",
      "  joint: 0 - Train model on Joint (single task)?\n",
      "  eval_future: False - Evaluate future tasks?\n",
      "  custom_task_order: None - Custom order of the tasks. The tasks are separated by commas or \"-\". Example: `--custom_task_order=0,1,2,3` or `--custom_task_order=0-3`.\n",
      "NOTE: this option is not supported by all datasets. If not provided, the tasks are ordered as in the dataset.\n",
      "NOTE: this option disables the `--permute_classes` option and is incompatible with `--custom_class_order`\n",
      "  custom_class_order: None - Custom order of the classes. The classes are separated by commas or \"-\". Example: `--custom_class_order=0,1,2,3` or `--custom_class_order=0-3`.\n",
      "NOTE: this option is not supported by all datasets. If not provided, the classes are ordered as in the dataset.\n",
      "NOTE: this option disables the `--permute_classes` option and is incompatible with `--custom_task_order`\n",
      "  validation: None - Percentage of samples FOR EACH CLASS drawn from the training set to build the validation set.\n",
      "  validation_mode: current - Mode used for validation. Must be used in combination with `validation` argument. Possible values: - `current`: uses only the current task for validation (default). - `complete`: uses data from both current and past tasks for validation.\n",
      "  fitting_mode: epochs - Strategy used for fitting the model. Possible values: - `epochs`: fits the model for a fixed number of epochs (default). NOTE: this option is controlled by the `n_epochs` argument. - `iters`: fits the model for a fixed number of iterations. NOTE: this option is controlled by the `n_iters` argument. - `early_stopping`: fits the model until early stopping criteria are met. This option requires a validation set (see `validation` argument).   The early stopping criteria are: if the validation loss does not decrease for `early_stopping_patience` epochs, the training stops.\n",
      "  early_stopping_patience: 5 - Number of epochs to wait before stopping the training if the validation loss does not decrease. Used only if `fitting_mode=early_stopping`.\n",
      "  early_stopping_metric: loss - Metric used for early stopping. Used only if `fitting_mode=early_stopping`.\n",
      "  early_stopping_freq: 1 - Frequency of validation evaluation. Used only if `fitting_mode=early_stopping`.\n",
      "  early_stopping_epsilon: 1e-06 - Minimum improvement required to consider a new best model. Used only if `fitting_mode=early_stopping`.\n",
      "  n_epochs: 50 - Number of epochs. Used only if `fitting_mode=epochs`.\n",
      "  n_iters: None - Number of iterations. Used only if `fitting_mode=iters`.\n",
      "  optimizer: sgd - Optimizer.\n",
      "  optim_wd: 0.0 - optimizer weight decay.\n",
      "  optim_mom: 0.0 - optimizer momentum.\n",
      "  optim_nesterov: False - optimizer nesterov momentum.\n",
      "  drop_last: False - Drop the last batch if it is not complete?\n",
      "  lr_scheduler: None - Learning rate scheduler.\n",
      "  scheduler_mode: epoch - Scheduler mode. Possible values: - `epoch`: the scheduler is called at the end of each epoch. - `iter`: the scheduler is called at the end of each iteration.\n",
      "  lr_milestones: [] - Learning rate scheduler milestones (used if `lr_scheduler=multisteplr`).\n",
      "  sched_multistep_lr_gamma: 0.1 - Learning rate scheduler gamma (used if `lr_scheduler=multisteplr`).\n",
      "  noise_type: symmetric - Type of noise to apply. The symmetric type is supported by all datasets, while the asymmetric must be supported explicitly by the dataset (see `datasets/utils/label_noise`).\n",
      "  noise_rate: 0.0 - Noise rate in [0-1].\n",
      "  disable_noisy_labels_cache: False - Disable caching the noisy label targets? NOTE: if the seed is not set, the noisy labels will be different at each run with this option disabled.\n",
      "  cache_path_noisy_labels: noisy_labels - Path where to save the noisy labels cache. The path is relative to the `base_path`.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The `get_avail_args` function returns a dictionary of available arguments for the model.\n",
    "The arguments are divided into required and optional ones.\n",
    "\n",
    "- The required arguments are those that MUST be provided to run the model.\n",
    "- The optional arguments are those that can be provided to customize the model's behavior (such as changing the batch_size or saving/loading a checkpoint).\n",
    "\"\"\"\n",
    "\n",
    "required_args, optional_args = get_avail_args(dataset='seq-cifar10', model='sgd')\n",
    "\n",
    "print(\"Required arguments:\")\n",
    "for arg, info in required_args.items():\n",
    "    print(f\"  {arg}: {info['description']}\")\n",
    "\n",
    "print(\"\\nOptional arguments:\")\n",
    "for arg, info in optional_args.items():\n",
    "    print(f\"  {arg}: {info['default']} - {info['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d73d1d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12-Jun-25 18:36:23 - Trying to load default configuration for model sgd but no configuration file found in None.\n",
      "[INFO] 12-Jun-25 18:36:24 - Using device cuda:0\n",
      "[INFO] 12-Jun-25 18:36:24 - `wandb_entity` and `wandb_project` not set. Disabling wandb.\n",
      "[INFO] 12-Jun-25 18:36:24 - Using backbone: resnet18\n",
      "[INFO] 12-Jun-25 18:36:24 - Using ResNet as backbone\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To load the necessary stuff to run a model on a particular dataset, we can use the `load_runner` function.\n",
    "This function takes the model name, dataset name, and a dictionary of arguments as input.\n",
    "The dictionary of arguments can contain both required and optional arguments.\n",
    "\n",
    "The `load_runner` function returns the model and dataset to be used for training.\n",
    "The model and dataset are already set up with the provided arguments.\n",
    "\"\"\"\n",
    "\n",
    "model, dataset = load_runner('sgd','seq-cifar10',{'lr': 0.1, 'n_epochs': 1, 'batch_size': 32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa108461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 12-Jun-25 18:37:05 - Using 8 workers for the dataloader.\n",
      "[INFO] 12-Jun-25 18:37:05 - Using 8 workers for the dataloader.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f9d0ec7fe44093a1dd973426385c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:SEND Error: Host unreachable\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We can now run the model on the dataset using the `train` function.\n",
    "\"\"\"\n",
    "\n",
    "train(model, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
